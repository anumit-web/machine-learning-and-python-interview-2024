{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anumit-web/python-interview-2024/blob/main/machine%20learning/Unsupervised_Learning_clustering_type_iris_flower_clustering_Gaussian_mixture_model_(GMM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8lzIrkY9qBJ"
      },
      "source": [
        "# Python interview preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing (NLP)"
      ],
      "metadata": {
        "id": "tomuVIUl9uY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1"
      ],
      "metadata": {
        "id": "Ow1vTaKl9oHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Ko4QaixAfPyF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Vn6fvLFJGH"
      },
      "source": [
        "# Machine Learning\n",
        "# Natural Language Processing (NLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is NLTK?\n",
        "NLTK is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
        "\n",
        "What can it do for you?\n"
      ],
      "metadata": {
        "id": "gJO7K5ra955w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "basic text preprocessing techniques that NLTK supports\n",
        "\n",
        "some of the advanced features of NLKT:\n",
        "\n",
        "1. Sentence detection\n",
        "\n",
        "2. Tokenization\n",
        "\n",
        "3. Stop word removal\n",
        "\n",
        "4. Part-of-speech tagging\n",
        "\n",
        "5. Word frequencies\n",
        "\n",
        "6. Lemmatization\n",
        "\n",
        "7. Chunking and Chinking"
      ],
      "metadata": {
        "id": "bZW63qaK-cBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "K4uUP85Ez_02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "3pRBaqLz9atZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TFXpB4wv-5-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6b5128-f44e-40a5-c74d-98c47dc955f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0djESK29att",
        "outputId": "6d44f86e-5d16-4d54-d81c-837b9d2ccc2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, World!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nfFMli_AEOK2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python code"
      ],
      "metadata": {
        "id": "MR391vUc7l-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Sentence Detection (Sentence Boundary Detection)"
      ],
      "metadata": {
        "id": "yKXoY6HoATUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Il5GhWFYEOsa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688af4a9-4e5c-4ab3-b965-6337641e574d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************\n",
            "Apple's name was inspired by Steve Jobs' visit.\n",
            "His visit was to an apple farm while on a fruitarian diet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')    # Use nltk downloader to download resource \"punkt\"\n",
        "output = (\"Apple's name was inspired by Steve Jobs' visit. His visit was to an apple farm while on a fruitarian diet.\")\n",
        "\n",
        "# Create a string object. By default, the function breaks sentences by periods.\n",
        "# Customize text or read files as needed\n",
        "\n",
        "# Tokenize output (sentence-level)\n",
        "sentences = sent_tokenize(output)\n",
        "\n",
        "print(\"**************\")\n",
        "for sentence in sentences:\n",
        "# Print each sentence in the output with one sentence a line\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tokenization"
      ],
      "metadata": {
        "id": "rOz3EqKqBiaW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "laFvPGe_EPHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d548cba-3f8b-4503-f824-efb6943196ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n",
            "'s\n",
            "name\n",
            "was\n",
            "inspired\n",
            "by\n",
            "Steve\n",
            "Jobs\n",
            "'\n",
            "visit\n",
            ".\n",
            "His\n",
            "visit\n",
            "was\n",
            "to\n",
            "an\n",
            "apple\n",
            "farm\n",
            "while\n",
            "on\n",
            "a\n",
            "fruitarian\n",
            "diet\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')    # Use nltk downloader to download resource \"punkt\"\n",
        "\n",
        "output = (\"Apple's name was inspired by Steve Jobs' visit. His visit was to an apple farm while on a fruitarian diet.\")\n",
        "# Create a string object. By default, the function breaks sentences by periods\n",
        "# Customize text or read files as needed\n",
        "\n",
        "# Tokenize output (word-level)\n",
        "words = word_tokenize(output)\n",
        "\n",
        "for word in words:\n",
        "# Print each sentence in the output with one sentence a line\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stop Words Removal"
      ],
      "metadata": {
        "id": "sVoNAN1UCzUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can identify and remove stop words by using NLTK's list of stop words after tokenizing the text."
      ],
      "metadata": {
        "id": "7NolmD_KDHkd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2d4r9-x8EPmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d7515f-90bc-42d5-9cd2-9c6914477869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple\n",
            "'s\n",
            "name\n",
            "inspired\n",
            "Steve\n",
            "Jobs\n",
            "'\n",
            "visit\n",
            ".\n",
            "His\n",
            "visit\n",
            "apple\n",
            "farm\n",
            "fruitarian\n",
            "diet\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')    # Use nltk downloader to download resource \"punkt\"\n",
        "\n",
        "output = (\"Apple's name was inspired by Steve Jobs' visit. His visit was to an apple farm while on a fruitarian diet.\")\n",
        "# Create a string object. By default, the function breaks sentences by periods.\n",
        "# Customize text or read files as needed\n",
        "\n",
        "# Get a list of stop words in English\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Print non-stop words\n",
        "words = word_tokenize(output)\n",
        "for word in words:\n",
        "    if word not in stop_words:\n",
        "        print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pM4D78FVEQFG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AC_KZzEdEQhJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[University of Pennsylvania] (https://guides.library.upenn.edu/penntdm/python/nltk)\n",
        "\n"
      ],
      "metadata": {
        "id": "WhOR6HoX9NYL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (Anaconda)",
      "language": "python",
      "name": "anaconda3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}